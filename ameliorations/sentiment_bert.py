"""
Module d'am√©lioration avec BERT pour la classification de sentiments
Utilise des embeddings pr√©-entra√Æn√©s pour am√©liorer les performances
"""

import json
import numpy as np
import pandas as pd
import joblib
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import torch

# Note: Installation requise
# pip install transformers torch

try:
    from transformers import CamembertTokenizer, CamembertModel
    BERT_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è  Biblioth√®que transformers non install√©e")
    print("Pour utiliser BERT : pip install transformers torch")
    BERT_AVAILABLE = False


class ClassificateurBERT:
    """Classe pour la classification de sentiments avec embeddings BERT"""
    
    def __init__(self, chemin_donnees_annotees, modele_bert='camembert-base'):
        """
        Initialise le classificateur BERT
        
        Args:
            chemin_donnees_annotees: Chemin vers les donn√©es annot√©es
            modele_bert: Nom du mod√®le BERT √† utiliser (CamemBERT pour le fran√ßais)
        """
        if not BERT_AVAILABLE:
            raise ImportError("Biblioth√®que transformers requise. Installez avec: pip install transformers torch")
        
        self.chemin_donnees = chemin_donnees_annotees
        self.modele_bert_nom = modele_bert
        self.encodeur = LabelEncoder()
        
        # Charger le mod√®le BERT et le tokenizer
        print(f"Chargement du mod√®le {modele_bert}...")
        self.tokenizer = CamembertTokenizer.from_pretrained(modele_bert)
        self.modele_bert = CamembertModel.from_pretrained(modele_bert)
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.modele_bert.to(self.device)
        self.modele_bert.eval()
        
        print(f"‚úì Mod√®le charg√© sur : {self.device}")
        
        # Dossier de sortie
        self.dossier_sortie = Path('ameliorations/resultats_bert')
        self.dossier_sortie.mkdir(parents=True, exist_ok=True)
    
    def charger_donnees(self):
        """Charge les donn√©es annot√©es"""
        print("\nChargement des donn√©es...")
        
        with open(self.chemin_donnees, 'r', encoding='utf-8') as f:
            donnees = json.load(f)
        
        # Filtrer les donn√©es annot√©es
        donnees_annotees = [item for item in donnees if 'sentiment' in item]
        
        if len(donnees_annotees) < 50:
            raise ValueError(
                f"Pas assez de donn√©es annot√©es ({len(donnees_annotees)}). "
                "Minimum requis : 50."
            )
        
        df = pd.DataFrame(donnees_annotees)
        print(f"‚úì {len(df)} publications annot√©es charg√©es")
        
        return df
    
    def generer_embeddings_bert(self, textes, batch_size=16):
        """
        G√©n√®re les embeddings BERT pour une liste de textes
        
        Args:
            textes: Liste de textes
            batch_size: Taille des batchs pour le traitement
        
        Returns:
            Matrice d'embeddings (numpy array)
        """
        print(f"\nG√©n√©ration des embeddings BERT pour {len(textes)} textes...")
        
        embeddings = []
        
        # Traiter par batches
        for i in range(0, len(textes), batch_size):
            batch_textes = textes[i:i+batch_size]
            
            # Tokeniser
            encoded = self.tokenizer(
                batch_textes,
                padding=True,
                truncation=True,
                max_length=512,
                return_tensors='pt'
            )
            
            # D√©placer sur le device
            input_ids = encoded['input_ids'].to(self.device)
            attention_mask = encoded['attention_mask'].to(self.device)
            
            # G√©n√©rer les embeddings
            with torch.no_grad():
                outputs = self.modele_bert(input_ids=input_ids, attention_mask=attention_mask)
                # Utiliser le [CLS] token (premier token) comme repr√©sentation
                cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
                embeddings.append(cls_embeddings)
            
            if (i // batch_size + 1) % 10 == 0:
                print(f"  Progression : {i+batch_size}/{len(textes)}")
        
        # Concat√©ner tous les embeddings
        embeddings = np.vstack(embeddings)
        print(f"‚úì Embeddings g√©n√©r√©s : {embeddings.shape}")
        
        return embeddings
    
    def preparer_donnees(self, df):
        """
        Pr√©pare les donn√©es avec embeddings BERT
        
        Args:
            df: DataFrame avec les donn√©es annot√©es
        
        Returns:
            X_train, X_test, y_train, y_test, embeddings
        """
        print("\nPr√©paration des donn√©es...")
        
        # Extraire les textes
        if 'texte_traite' in df.columns and df['texte_traite'].notna().all():
            textes = df['texte_traite'].tolist()
        else:
            textes = df['texte'].tolist()
        
        # G√©n√©rer les embeddings
        embeddings = self.generer_embeddings_bert(textes)
        
        # Sauvegarder les embeddings
        np.save(self.dossier_sortie / 'bert_embeddings.npy', embeddings)
        print(f"‚úì Embeddings sauvegard√©s dans {self.dossier_sortie}/bert_embeddings.npy")
        
        # Encoder les labels
        y = self.encodeur.fit_transform(df['sentiment'])
        
        # Division train/test
        X_train, X_test, y_train, y_test = train_test_split(
            embeddings, y, test_size=0.2, random_state=42, stratify=y
        )
        
        print(f"‚úì Taille ensemble d'entra√Ænement : {X_train.shape[0]}")
        print(f"‚úì Taille ensemble de test : {X_test.shape[0]}")
        
        return X_train, X_test, y_train, y_test, embeddings
    
    def entrainer_modeles(self, X_train, y_train):
        """
        Entra√Æne plusieurs mod√®les sur les embeddings BERT
        
        Args:
            X_train: Embeddings d'entra√Ænement
            y_train: Labels d'entra√Ænement
        
        Returns:
            Dictionnaire des mod√®les entra√Æn√©s
        """
        print("\n" + "="*60)
        print("ENTRA√éNEMENT DES MOD√àLES AVEC EMBEDDINGS BERT")
        print("="*60)
        
        modeles = {
            'R√©gression Logistique': LogisticRegression(max_iter=1000, random_state=42),
            'SVM RBF': SVC(kernel='rbf', random_state=42, probability=True),
            'Random Forest': RandomForestClassifier(n_estimators=200, random_state=42)
        }
        
        resultats = {}
        
        for nom, modele in modeles.items():
            print(f"\nüìä Entra√Ænement : {nom}")
            modele.fit(X_train, y_train)
            score = modele.score(X_train, y_train)
            print(f"  Score d'entra√Ænement : {score:.4f}")
            
            resultats[nom] = modele
        
        print("\n" + "="*60)
        
        return resultats
    
    def evaluer_modeles(self, modeles, X_test, y_test):
        """
        √âvalue les mod√®les sur l'ensemble de test
        
        Args:
            modeles: Dictionnaire des mod√®les
            X_test: Embeddings de test
            y_test: Labels de test
        
        Returns:
            DataFrame avec les r√©sultats, meilleur mod√®le
        """
        print("\n" + "="*60)
        print("√âVALUATION SUR L'ENSEMBLE DE TEST")
        print("="*60)
        
        resultats = []
        meilleur_score = 0
        meilleur_modele = None
        meilleur_nom = None
        
        for nom, modele in modeles.items():
            y_pred = modele.predict(X_test)
            score = accuracy_score(y_test, y_pred)
            
            print(f"\n{nom} :")
            print(f"  Exactitude : {score:.4f}")
            print(classification_report(
                y_test, y_pred,
                target_names=self.encodeur.classes_,
                zero_division=0
            ))
            
            resultats.append({
                'Mod√®le': f'BERT + {nom}',
                'Exactitude': score
            })
            
            # Matrice de confusion
            cm = confusion_matrix(y_test, y_pred)
            self._sauvegarder_matrice_confusion(cm, nom, y_test, y_pred)
            
            if score > meilleur_score:
                meilleur_score = score
                meilleur_modele = modele
                meilleur_nom = nom
        
        print("\n" + "="*60)
        print(f"üèÜ MEILLEUR MOD√àLE : BERT + {meilleur_nom}")
        print(f"   Exactitude : {meilleur_score:.4f}")
        print("="*60)
        
        df_resultats = pd.DataFrame(resultats)
        df_resultats.to_csv(self.dossier_sortie / 'resultats_bert.csv', index=False)
        
        return df_resultats, meilleur_modele, meilleur_nom, meilleur_score
    
    def _sauvegarder_matrice_confusion(self, cm, nom_modele, y_test, y_pred):
        """Sauvegarde la matrice de confusion"""
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=self.encodeur.classes_,
                   yticklabels=self.encodeur.classes_)
        plt.title(f'Matrice de Confusion - BERT + {nom_modele}')
        plt.ylabel('V√©rit√© terrain')
        plt.xlabel('Pr√©dictions')
        plt.tight_layout()
        
        filename = f'matrice_confusion_bert_{nom_modele.replace(" ", "_").lower()}.png'
        plt.savefig(self.dossier_sortie / filename, dpi=300, bbox_inches='tight')
        plt.close()
    
    def sauvegarder_modele(self, modele, nom_modele, score):
        """Sauvegarde le meilleur mod√®le BERT"""
        # Sauvegarder le mod√®le
        joblib.dump(modele, self.dossier_sortie / 'modele_bert.pkl')
        joblib.dump(self.encodeur, self.dossier_sortie / 'encodeur_bert.pkl')
        
        # M√©tadonn√©es
        metadata = {
            'modele_bert': self.modele_bert_nom,
            'classificateur': nom_modele,
            'exactitude': float(score),
            'classes': self.encodeur.classes_.tolist(),
            'date_entrainement': datetime.now().isoformat()
        }
        
        with open(self.dossier_sortie / 'metadata_bert.json', 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        print(f"\n‚úÖ Mod√®le BERT sauvegard√© dans {self.dossier_sortie}")
    
    def comparer_avec_tfidf(self):
        """Compare les performances avec le mod√®le TF-IDF"""
        print("\n" + "="*60)
        print("COMPARAISON BERT vs TF-IDF")
        print("="*60)
        
        # Charger les r√©sultats TF-IDF si disponibles
        chemin_tfidf = Path('EntrainementModele/resultats/comparaison_modeles.csv')
        chemin_bert = self.dossier_sortie / 'resultats_bert.csv'
        
        if chemin_tfidf.exists() and chemin_bert.exists():
            df_tfidf = pd.read_csv(chemin_tfidf)
            df_bert = pd.read_csv(chemin_bert)
            
            print("\nMeilleurs scores TF-IDF :")
            print(df_tfidf[['Mod√®le', 'Score Test']].to_string(index=False))
            
            print("\nScores BERT :")
            print(df_bert[['Mod√®le', 'Exactitude']].to_string(index=False))
            
            # Visualisation comparative
            self._visualiser_comparaison_bert_tfidf(df_tfidf, df_bert)
        else:
            print("Fichiers de r√©sultats TF-IDF non trouv√©s pour la comparaison")
        
        print("="*60)
    
    def _visualiser_comparaison_bert_tfidf(self, df_tfidf, df_bert):
        """Cr√©e une visualisation comparative BERT vs TF-IDF"""
        plt.figure(figsize=(12, 6))
        
        # Pr√©parer les donn√©es
        max_score_tfidf = df_tfidf['Score Test'].max()
        max_score_bert = df_bert['Exactitude'].max()
        
        categories = ['TF-IDF (Meilleur)', 'BERT (Meilleur)']
        scores = [max_score_tfidf, max_score_bert]
        colors = ['skyblue', 'lightcoral']
        
        bars = plt.bar(categories, scores, color=colors, alpha=0.7, edgecolor='black')
        
        # Ajouter les valeurs sur les barres
        for bar, score in zip(bars, scores):
            height = bar.get_height()
            plt.text(bar.get_x() + bar.get_width()/2., height,
                    f'{score:.2%}',
                    ha='center', va='bottom', fontsize=12, fontweight='bold')
        
        plt.ylabel('Exactitude (Accuracy)')
        plt.title('Comparaison des Performances : TF-IDF vs BERT')
        plt.ylim(0, 1)
        plt.grid(axis='y', alpha=0.3)
        plt.tight_layout()
        
        plt.savefig(self.dossier_sortie / 'comparaison_bert_tfidf.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"\nüìä Graphique de comparaison sauvegard√©")


def main():
    """Fonction principale"""
    if not BERT_AVAILABLE:
        print("‚ùå Biblioth√®que transformers non install√©e")
        print("Installation : pip install transformers torch")
        return
    
    print("="*60)
    print("AM√âLIORATION AVEC BERT - CLASSIFICATION DE SENTIMENTS")
    print("="*60)
    
    # Utiliser la nouvelle structure
    script_dir = Path(__file__).parent
    projet_dir = script_dir.parent
    chemin_donnees = projet_dir / '02_Annotation' / 'uvbf_data_annote.json'
    
    if not chemin_donnees.exists():
        print(f"\n‚ùå Erreur : Fichier {chemin_donnees} non trouv√©")
        print("Veuillez d'abord annoter les donn√©es.")
        return
    
    try:
        # Cr√©er le classificateur
        classificateur = ClassificateurBERT(chemin_donnees)
        
        # Charger les donn√©es
        df = classificateur.charger_donnees()
        
        # Pr√©parer les donn√©es avec BERT
        X_train, X_test, y_train, y_test, embeddings = classificateur.preparer_donnees(df)
        
        # Entra√Æner les mod√®les
        modeles = classificateur.entrainer_modeles(X_train, y_train)
        
        # √âvaluer les mod√®les
        df_resultats, meilleur_modele, meilleur_nom, meilleur_score = classificateur.evaluer_modeles(
            modeles, X_test, y_test
        )
        
        # Sauvegarder le meilleur mod√®le
        classificateur.sauvegarder_modele(meilleur_modele, meilleur_nom, meilleur_score)
        
        # Comparer avec TF-IDF
        classificateur.comparer_avec_tfidf()
        
        print("\n‚úÖ AM√âLIORATION BERT TERMIN√âE !")
        print(f"üìÇ R√©sultats dans : {classificateur.dossier_sortie}")
        
    except Exception as e:
        print(f"\n‚ùå Erreur : {str(e)}")
        raise


if __name__ == "__main__":
    main()


